dimnames(trn.val.tst$training_set)
class(trn.val.tst$training_set)
class(trn.val.tst$validation_set)
trn.val.tst <- myStandardPartitioning(dt)
class(trn.val.tst$validation_set)
class(trn.val.tst$training_set)
createDummies <- function(df, resp) {
fmla <- as.formula(paste0(resp, "~."))
dummify <- dummyVars(fmla,data = df)
return(as.data.frame(predict(dummify, df)))
}
trn.val.tst$training_set <- createDummies(trn.val.tst$training_set, dt$resp)
class(trn.val.tst$training_set)
head(trn.val.tst$training_set)
head(trn.val.tst$training_set$color..L)
head(trn.val.tst$validation_set$color)
histogram(trn.val.tst$training_set$color..L)
dotplot(trn.val.tst$training_set$color..L)
dotplot(~trn.val.tst$training_set$color..L)
rm(list = ls())
createModels <- function(models) {
n <- nrow(models)
for (m in 1:n) {
assign(models[m,"model"]
, get_or_train(algo = models[m,"algo"]
, trans = models[m, "trans"]
, tc = models[m, "tCtrl"])
)
}
}
trainControl(method = "cv", number = 5, allowParallel = TRUE)
# stat learning project set up
source("utilityCode.R")
# stat learning project set up
source("utilityCode.R")
# choose your statistical learning method
algorithms <- c("gbm", "qda", "rf")
# list your transforms or just set for full set.
transforms <- c("pca", "set")
# set up the models matrix
models <- createModelMatrix(algorithms, transforms)
tCtrls <- data.frame(algorithms = algorithms
, tCtrl = trainControl(method = "cv", number = 5, allowParallel = TRUE))
View(models)
tCtrls <- data.frame(algorithms = algorithms
, rep(tCtrl = trainControl(method = "cv", number = 5, allowParallel = TRUE)
, length(algorithms)))
View(tCtrls)
tCtrls <- data.frame(algorithms = algorithms
, tCtrl = rep(trainControl(method = "cv", number = 5, allowParallel = TRUE)
, length(algorithms)
)
)
tCtrl = rep(trainControl(method = "cv", number = 5, allowParallel = TRUE)
, length(algorithms)
)
tCtrl = trainControl(method = "cv", number = 5, allowParallel = TRUE)
tCtrls <- list()
tCtrls <- list()
tCtrls[[algorithms]] <- 1:length(algorithms)
tCtrls[[algorithms]] <- 1
tCtrls[[algorithms[1]]] <- 1
tCtrls <- list()
for (algo in algorithms) {
tCtrls[[algo]] <- tCtrl = trainControl(method = "cv", number = 5, allowParallel = TRUE)
}
tCtrls <- list()
for (algo in algorithms) {
tCtrls[[algo]] <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
}
data("diamonds")
dt <- setData(diamonds, "cut")
for (var in dt$vars[!dt$vars_fac]) {
v <- myViolinPlot(var, dt)
print(v)
}
trn.val.tst$training_set <- createDummies(trn.val.tst$training_set, dt$resp)
dt <- setData(diamonds, "cut")
dt$dt.frm <- createDummies(dt$dt.frm, dt$resp)
dt$dt.frm
head(dt$dt.frm)
data("diamonds")
dt <- setData(diamonds, "cut")
na.vals.check(dt)
# and near zero variance
nzv.check(dt)
cor.vars.check(dt, 0.8)
lin.comb.check(dt)
trn.val.tst <- myStandardPartitioning(dt)
algo <- "lda"
trans <- "set"
df <- dt$dt.frm
resp <- dt$resp
tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)
library)caret
library(caret)
tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)
modelName <- paste(algo, trans, sep = "_")
modelFileName <- paste0("model_", modelName, ".RData")
names(dt)
if (file.exists(modelFileName)) {
attach(modelFileName, warn.conflicts = FALSE)
} else {
dsName <- paste0("training_", trans)
}
p_clus <- makeCluster(detectCores())
library(parallel)
library(doParallel)
p_clus <- makeCluster(detectCores())
registerDoParallel(p_clus)
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = get(dsName),  trControl = tc, method = algo))
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = df,  trControl = tc, method = algo))
stopCluster(p_clus)
model <- get(modelName)
save(model, file = modelFileName)
save(get(modelName), file = modelFileName)
save((get(modelName)), file = modelFileName)
tCtrls$rf
models
tCtrls[[algo]]
algo
algo <- "qda"
tCtrls[[algo]]
tCtrls[algo]
# beautiful boiler plate for creating the models
get_or_train <- function(df, resp, algo, trans, tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)) {
modelName <- paste(algo, trans, sep = "_")
modelFileName <- paste0("model_", modelName, ".RData")
if (file.exists(modelFileName)) {
attach(modelFileName, warn.conflicts = FALSE)
} else {
dsName <- paste0("training_", trans)
# set up parallel processing
p_clus <- makeCluster(detectCores())
registerDoParallel(p_clus)
# build the model
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = df,  trControl = tc, method = algo))
# close parallel processing
stopCluster(p_clus)
# naive cache
# save out to an external file for re-use
model <- get(modelName)
save(model, file = modelFileName)
}
return(model)
}
createModels <- function(df, resp, models, tCtrls) {
n <- nrow(models)
for (m in 1:n) {
assign(models[m,"model"]
, get_or_train(algo = models[m,"algo"]
, trans = models[m, "trans"]
, tc = tCtrls[[algo]])
)
}
}
get_or_train(df, resp, "lda", "set")
get_or_train(df, resp, "qda", "set")
algorithms <- c("qda", "lda")
# list your transforms or just set for full set.
transforms <- c("set")
models <- createModelMatrix(algorithms, transforms)
tCtrls <- list()
for (algo in algorithms) {
tCtrls[[algo]] <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
}
re(list=ls())
rm(list=ls())
# stat learning project set up
source("utilityCode.R")
# choose your statistical learning method
algorithms <- c("qda", "lda")
# list your transforms or just set for full set.
transforms <- c("set")
# set up the models matrix
models <- createModelMatrix(algorithms, transforms)
# set up the train controls for each model
# to customise for any model, over-write the default
tCtrls <- list()
for (algo in algorithms) {
tCtrls[[algo]] <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
}
data("diamonds")
dt <- setData(diamonds, "cut")
# partition the data here for modeling and validation
trn.val.tst <- myStandardPartitioning(dt)
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
trn.val.tst[[model_name]]
trn.val.tst[[modelName]]
modelName <- paste(algo, trans, sep = "_")
modelFileName <- paste0("model_", modelName, ".RData")
trn.val.tst[["training_set"]]
get(paste0("trn.val.test[[", "training_set", "]]")
)
modelName <- "training_set"
get(paste0("trn.val.test[[", modelName, "]]"))
get(paste0("trn.val.test[[\"", modelName, "\"]]"))
# to be generic - set the data set names here
setData <- function(df, resp, makeFactorResp = FALSE) {
if (makeFactorResp) { # if resp should be a factor but is not one
df[[resp]] <- factor(df[[resp]])
isFactorResp <- TRUE
}
num_classes <- NA
if (any(class(df[[resp]]) == "factor")) {
num_classes <- length(levels(factor(df[[resp]])))
}
respCol = getRespCol(df, resp)
dt <- list(dt.frm = df
, resp = resp
, num_classes = num_classes
, vars = names(df)[-respCol]
, vars_fac = sapply(df[-respCol], function(j) { any(class(j) == "factor") } )
, respCol = respCol
)
class(dt) <- dt_collection
return(dt)
}
data("diamonds")
dt <- setData(diamonds, "cut")
setData <- function(df, resp, makeFactorResp = FALSE) {
if (makeFactorResp) { # if resp should be a factor but is not one
df[[resp]] <- factor(df[[resp]])
isFactorResp <- TRUE
}
num_classes <- NA
if (any(class(df[[resp]]) == "factor")) {
num_classes <- length(levels(factor(df[[resp]])))
}
respCol = getRespCol(df, resp)
dt <- list(dt.frm = df
, resp = resp
, num_classes = num_classes
, vars = names(df)[-respCol]
, vars_fac = sapply(df[-respCol], function(j) { any(class(j) == "factor") } )
, respCol = respCol
)
attr(dt, "class") <- dt_collection
return(dt)
}
data("diamonds")
dt <- setData(diamonds, "cut")
setData <- function(df, resp, makeFactorResp = FALSE) {
if (makeFactorResp) { # if resp should be a factor but is not one
df[[resp]] <- factor(df[[resp]])
isFactorResp <- TRUE
}
num_classes <- NA
if (any(class(df[[resp]]) == "factor")) {
num_classes <- length(levels(factor(df[[resp]])))
}
respCol = getRespCol(df, resp)
dt <- list(dt.frm = df
, resp = resp
, num_classes = num_classes
, vars = names(df)[-respCol]
, vars_fac = sapply(df[-respCol], function(j) { any(class(j) == "factor") } )
, respCol = respCol
)
class(dt) <- "dt_collection"
return(dt)
}
data("diamonds")
dt <- setData(diamonds, "cut")
class(dt$dt.frm)
models
algorithms <- c("blassoAveraged", "enet")
# list your transforms or just set for full set.
transforms <- c("trn")
# set up the models matrix
models <- createModelMatrix(algorithms, transforms)
models
createModelMatrix <- function(algorithms, dataset) {
models <- cbind(algo = rep(algorithms, each = 2), set = dataset)
models <- cbind(models
, model = paste(models[,1]
, models[,2]
,"model", sep = "_")
)
models <- cbind(models
, result = gsub("model", "cm", models[,3])
, pred = gsub("model", "pred", models[,3])
, mark = gsub("model", "mark", models[,3])
)
return(models)
}
algorithms <- c("blassoAveraged", "enet")
# list your transforms or just set for full set.
sets <- c("trn")
# set up the models matrix
models <- createModelMatrix(algorithms, sets)
models
createModels <- function(df, resp, models, tCtrls) {
n <- nrow(models)
if (class(df) == "data.frame") {
for (m in 1:n) {
assign(models[m,"model"]
, get_or_train(algo = models[m,"algo"]
, set = models[m, "set"]
, tc = tCtrls[[algo]])
)
}
}
}
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
# beautiful boiler plate for creating the models
get_or_train <- function(df, resp, algo, set = "trn"
, tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)) {
modelName <- paste(algo, dsName, sep = "_")
modelFileName <- paste0(modelName, ".RData")
if (file.exists(modelFileName)) {
attach(modelFileName, warn.conflicts = FALSE)
} else {
# set up parallel processing
p_clus <- makeCluster(detectCores())
registerDoParallel(p_clus)
# build the model
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = df,  trControl = tc, method = algo))
# close parallel processing
stopCluster(p_clus)
# naive cache
# save out to an external file for re-use
model <- get(modelName)
save(model, file = modelFileName)
}
return(model)
}
createModels <- function(df, resp, models, tCtrls) {
n <- nrow(models)
if (class(df) == "data.frame") {
for (m in 1:n) {
assign(models[m,"model"]
, get_or_train(algo = models[m,"algo"]
, set = models[m, "set"]
, tc = tCtrls[[algo]])
)
}
}
}
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
get_or_train <- function(df, resp, algo, set = "trn"
, tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)) {
modelName <- paste(algo, set, sep = "_")
modelFileName <- paste0(modelName, ".RData")
if (file.exists(modelFileName)) {
attach(modelFileName, warn.conflicts = FALSE)
} else {
# set up parallel processing
p_clus <- makeCluster(detectCores())
registerDoParallel(p_clus)
# build the model
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = df,  trControl = tc, method = algo))
# close parallel processing
stopCluster(p_clus)
# naive cache
# save out to an external file for re-use
model <- get(modelName)
save(model, file = modelFileName)
}
return(model)
}
get_or_train <- function(df, resp, algo, set = "trn"
, tc = trainControl(method = "cv"
, number = 5
, allowParallel = TRUE)) {
modelName <- paste(algo, set, sep = "_")
modelFileName <- paste0(modelName, ".RData")
if (file.exists(modelFileName)) {
attach(modelFileName, warn.conflicts = FALSE)
} else {
# set up parallel processing
p_clus <- makeCluster(detectCores())
registerDoParallel(p_clus)
# build the model
fmla <- as.formula(paste0(resp, "~."))
assign(modelName, train(fmla, data = df,  trControl = tc, method = algo))
# close parallel processing
stopCluster(p_clus)
# naive cache
# save out to an external file for re-use
model <- get(modelName)
save(model, file = modelFileName)
}
return(model)
}
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
createModels <- function(df, resp, models, tCtrls) {
n <- nrow(models)
if (class(df) == "data.frame") {
for (m in 1:n) {
assign(models[m,"model"]
, get_or_train(df, resp
, algo = models[m,"algo"]
, set = models[m, "set"]
, tc = tCtrls[[algo]])
)
}
}
}
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
modelType
algorithms <- c("lda", "qda")
# list your transforms or just set for full set.
sets <- c("trn")
# set up the models matrix
models <- createModelMatrix(algorithms, sets)
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
a <- createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
models
models <- cbind(algo = rep(algorithms, each = length(dataset)), set = dataset)
dataset <- "trn"
models <- cbind(algo = rep(algorithms, each = length(dataset)), set = dataset)
models
models <- cbind(models
, model = paste(models[,1]
, models[,2]
,"model", sep = "_")
)
models
models <- cbind(models
, result = gsub("model", "cm", models[,3])
, pred = gsub("model", "pred", models[,3])
, mark = gsub("model", "mark", models[,3])
)
models
sets <- c("trn", "pca")
models <- cbind(algo = rep(algorithms, each = length(sets)), set = sets)
models <- cbind(models
, model = paste(models[,1]
, models[,2]
,"model", sep = "_")
)
models <- cbind(models
, result = gsub("model", "cm", models[,3])
, pred = gsub("model", "pred", models[,3])
, mark = gsub("model", "mark", models[,3])
)
models
createModels(trn.val.tst$training_set, dt$resp, models, tCtrls)
rm(list=ls())
# stat learning project set up
source("utilityCode.R")
layoutPlots_4 <- function(vars, plotFunc, df, ...) {
print(plotFunc(vars[1], df), pos = c(0,0.5, 0.5, 1), more = TRUE)
print(plotFunc(vars[2], df), pos = c(0.5, 0.5, 1, 1), more = TRUE)
print(plotFunc(vars[3], df), pos = c(0, 0, 0.5, 0.5), more = TRUE)
print(plotFunc(vars[4], df), pos = c(0.5, 0, 1, 0.5))
}
vars <- c("x", "y", "z", "carat")
layoutPlots_4 <- function(vars, plotFunc, df, ...) {
print(plotFunc(vars[1], df, ...), pos = c(0,0.5, 0.5, 1), more = TRUE)
print(plotFunc(vars[2], df, ...), pos = c(0.5, 0.5, 1, 1), more = TRUE)
print(plotFunc(vars[3], df, ...), pos = c(0, 0, 0.5, 0.5), more = TRUE)
print(plotFunc(vars[4], df, ...), pos = c(0.5, 0, 1, 0.5))
}
data("diamonds")
dt <- setData(diamonds, "cut")
# stat learning project set up
source("utilityCode.R")
librarydata("diamonds")
dt <- setData(diamonds, "cut")
data("diamonds")
dt <- setData(diamonds, "cut")
layoutPlots_4(vars, myViolinPlot, dt$dt.frm, resp = dt$resp)
# partition the data here for modeling and validation
trn.val.tst <- myStandardPartitioning(dt)
myPreProc <- preProcess(trn.val.tst$trn[-dt$respCol]
, method = "pca"
, thresh = 0.9)
trn.val.tst$pca <- predict(myPreProc, trn.val.tst$trn)
dim(trn.val.tst$pca)[2] -1
# define models, transforms and custom code here
# choose your statistical learning method
algorithms <- c("lda", "qda")
# list your transforms or just "trn" for the as is training set
# and remember to build any transformed sets with custom code
# to avoid building unecesary duplicates
sets <- c("trn", "pca")
# set up the models matrix
models <- createModelMatrix(algorithms, sets)
# set up the train controls for each model
# to customise for any model, over-write the default
tCtrls <- list()
for (algo in algorithms) {
tCtrls[[algo]] <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
}
createModels(trn.val.tst, dt$resp, models, tCtrls)
lda_pca_model
lda_trn_model
qda_trn_model
qda_set_model
qda_pcs_model
qda_pca_model
plot(trn.val.tst$val, predict(qda_trn_model, trn.val.tst$val)
)
confusionMatrix(trn.val.tst$val$cut, predict(qda_trn_model, trn.val.tst$val)
)
