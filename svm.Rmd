# Support Vector Machines

## Support vector classifiers and separating planes

First set up some sim data

```{r}
library(e1071)
set.seed(1)
x <- matrix(rnorm(40), 20, 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y==1,] <- x[y==1,]+1
```

Check if the classes are linearly separable.

```{r}
plot(x, col = y + 3, pch = 19)
```

They are not.

svm function needs a factor to perform classification.

```{r}
dat <- data.frame(x, y = as.factor(y))
svmfit <- svm(y~., data = dat
              , kernel = "linear"
              , cost = 10, scale = FALSE)
plot(svmfit, dat)
```

The plot is horrible

The output has some parameters

```{r}
svmfit$index
summary(svmfit)
```

A smaller cost can be applied:

```{r}
dat = data.frame(x, y = as.factor(y))
svmfit <- svm(y~., data = dat
              , kernel = "linear"
              , cost = 0.1, scale = FALSE)
plot(svmfit, dat)

svmfit$index
summary(svmfit)
```

The tune function can be used to return cross validated models to pick the best params

```{r}
set.seed(1)
tune.out=tune(svm, y∼., data=dat
              , kernel="linear"
              , ranges=list(
                cost=c(0.001, 0.01
                       , 0.1, 1,5
                       ,10, 100)
                )
              )


summary(tune.out)
```

tune stores the best model as follows:
```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

Can use the model to predict
```{r}
xtest <- matrix(rnorm (20*2) , ncol=2)
ytest <- sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat <- data.frame(xtest, y=as.factor(ytest))
```

predict and create a conf mat

```{r}
ypred=predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
```

What does it look like with 0.01 as cost?

```{r}
svmfit <- svm(y~., data = dat
              , kernel = "linear"
              , cost = 0.01, scale = FALSE)
plot(svmfit, dat)

ypred=predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

Example where the classes are linearly separable:

```{r}
x[y==1,] <- x[y==1,]+0.5
plot(x, col = y+3, pch = 19)
dat <- data.frame(x, y = as.factor(y))
```

The classes are barely inseparable. Fitting with a very large value for cost ensures that none are misclassified.

```{r}
svmfit <- svm(y~., data = dat
              , kernel = "linear"
              , cost = 10e5, scale = FALSE)
plot(svmfit, dat)
summary(svmfit)
```

Non support vectors seen very close to decisions boundary. Suspect the model will perform poorly.

```{r}
svmfit <- svm(y~., data = dat
              , kernel = "linear"
              , cost = 1, scale = FALSE)
plot(svmfit, dat)
summary(svmfit)
```

Reducing cost to one misclassifies one point but uses 7 support vectors.

## Support vector machine
Set up some data with a nonlinear boundary

```{r}
set.seed(1)
x <- matrix(rnorm (400), ncol=2)
x[1:100,] <- x[1:100,]+2
x[101:150,] <- x[101:150,]-2
y <- c(rep(1,150),rep(2,50))
dat <- data.frame(x,y=as.factor(y))
plot(x, col = y, pch = 19)
```

Creating SVM with radial kernel

```{r}
train <- sample(200, 100)
svmfit <- svm(y~., data = dat[train,]
              , kernel = "radial"
              , cost = 1, gamma = 1
              , scale = FALSE)
plot(svmfit, dat[train,])
summary(svmfit)
```

There are a few training errors so we could reduce the cost but this comes at the price of a more irregular boundary which could be over fit.

```{r}
svmfit <- svm(y~., data = dat[train,]
              , kernel = "radial"
              , cost = 1e5, gamma = 1
              , scale = FALSE)
plot(svmfit, dat[train,])
summary(svmfit)
```

Can use tune to cross validate for the best parameter settings:

```{r}
set.seed(1)
tune.out <- tune(svm, y∼.
                 , data = dat[train ,]
                 , kernel="radial"
                 , ranges=list(
                   cost=c(0.1,1,10
                          ,100,1000)
                   , gamma=c(0.5,1
                             ,2,3,4)
                   )
                 )
summary(tune.out)
```

Using the best model, can predict on test data:

```{r}
confmat <- table(true=dat[-train ,"y"]
      , pred=predict(tune.out$best.model
                     , newx=dat[-train,]))
confmat
1 - (confmat[1,1]+confmat[2,2])/sum(confmat)
```

# ROC Plots

```{r}
library(ROCR)
```

Need to create a function for this:

```{r}
rocplot <- function (pred , truth , ...) {
  predob <- prediction(pred, truth)
  perf <- performance (predob, "tpr", "fpr")
  plot(perf ,...)
}
```

Using decision values outputs the values that lead to classification, rather than the classification itself.

```{r}
svmfit.opt <- svm(y∼., data=dat[train ,]
                  , kernel ="radial"
                  , gamma=2, cost=1
                  , decision.values=TRUE)

fitted <- attributes(predict
                     (svmfit.opt
                        , dat[train,]
                        , decision.values=TRUE)
                      )$decision.values

par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")

svmfit.flex <- svm(y∼., data=dat[train ,]
                  , kernel ="radial"
                  , gamma=50, cost=1
                  , decision.values=TRUE)

fitted <- attributes(predict
                     (svmfit.flex
                        , dat[train,]
                        , decision.values=TRUE)
                      )$decision.values


rocplot(fitted,dat[train,"y"],add=T,col="red")
```

However on the training data

```{r}
fitted <- attributes(predict
                     (svmfit.opt
                        , dat[-train,]
                        , decision.values=TRUE)
                      )$decision.values

rocplot(fitted,dat[-train,"y"],main="Training Data")

fitted <- attributes(predict
                     (svmfit.flex
                        , dat[-train,]
                        , decision.values=TRUE)
                      )$decision.values

rocplot(fitted,dat[-train,"y"],add=T,col="red")

par(mfrow=c(1,1))
```

# Multiclass scenario

```{r}
set.seed(1)
x <- matrix(rnorm (400), ncol=2)
x[1:100,] <- x[1:100,]+2
x[101:150,] <- x[101:150,]-2
y <- c(rep(1,150),rep(2,50))

x <- rbind(x, matrix(rnorm(100), ncol = 2))
y <- c(y, rep(0,50))
x[y==0,2] <- x[y==0,2] + 2
plot(x, col = y + 1, pch = 19)
```

Now perform the svm

```{r}
dat <- data.frame(x, y = as.factor(y))
svmfit <- svm(y∼., data=dat
              , kernel ="radial"
              , cost=10, gamma=1)
plot(svmfit, dat)
```

# gene expression

```{r}
library(e1071)
library(ISLR)
str(Khan)
```

This dataset has p >> n so a linear SVM is a suitable choice.

```{r}
dat <- data.frame(x=Khan$xtrain
                  , y=as.factor(Khan$ytrain ))
out <- svm(y∼., data=dat , kernel ="linear",cost=10)
summary(out)
table(out$fitted, dat$y)
```

There are no training errors. Unsurprising as with a large p there are many ways to find supporting hyperplanes.

```{r}
dat.te <- data.frame(x=Khan$xtest
                     , y=as.factor(Khan$ytest ))
pred.te <- predict(out
                   , newdata=dat.te)
table(pred.te, dat.te$y)
```

# Question 1
```{r}
x1 <- seq(-100, 100, 1)
x2 <- x1
myGrid <- expand.grid(x1 = x1, x2 = x2)

sep.plane1 <- function(x_1, x_2) {
  c <- 1 + 3 * x_1 - x_2
  if (c == 0) return("black")
  if (c > 0) return("yellow")
  if (c < 0) return("wheat")
}

sep.plane2 <- function(x_1, x_2) {
  c <- -2 + x_1 + 2 * x_2
  if (c == 0) return("black")
  if (c > 0) return("pink")
  if (c < 0) return("orange")
}


sp1 <- mapply(sep.plane1, myGrid[,1], myGrid[,2])
sp2 <- mapply(sep.plane2, myGrid[,1], myGrid[,2])
plot(myGrid, pch = ".", col = sp1)
points(myGrid, pch = 1, col = sp2)
```

# try a non linear boundary

```{r}
sep.plane3 <- function(x_1, x_2) {
  c <- (1 + x_1)^2 + (2 - x_2)^2
  if (c == 4) return("black")
  if (c > 4) return("pink")
  if (c < 4) return("orange")
}

x1 <- seq(-5, 5, 1)
x2 <- x1
myGrid <- expand.grid(x1 = x1, x2 = x2)

sp3 <- mapply(sep.plane3, myGrid[,1], myGrid[,2])
plot(myGrid, pch = 19, col = sp3, cex = 2)
points(0,0, col = "red")
points(-1,1, col = "red")
points(2,2, col = "red")
```

```{r}
x1 <- c(3,2,4,1,2,4,4)
x2 <- c(4,2,4,4,1,3,1)
y <- c(rep("red", 4), rep("blue", 3))
plot(x1, x2, col = y)
abline(-0.5, 1) # 0.5 - x1 + x2 = 0
lines(c(2,2.25), c(2,1.75))
lines(c(2,1.75), c(1,1.25))
abline(0,1, lty = 3)
abline(-1, 1, lty = 3)
```

# Q4
```{r}
# function to make a grid around the existing points
make.grid <- function(x,n=75){
  grange <- apply(x,2,range)
  x1 <- seq(from=grange[1,1],to=grange[2,1],length=n)
  x2 <- seq(from=grange[1,2],to=grange[2,2],length=n)
  expand.grid(x.1=x1,x.2=x2)
}

# toy data
set.seed(12345)
x <- matrix(rnorm(200), ncol = 2)
y <- rep(c("pink", "orange"), each = 50)
x[1:10,1] <- x[1:10,1] + 4
x[11:25,] <- x[11:25,]^3
x[26:50,2] <- x[26:50,2] * -4

dt <- data.frame(x = x, y = as.factor(y))
xgrid <- make.grid(x)

set.seed(12345)
train <- sample(c(TRUE, FALSE), 100, replace = TRUE)

x.train <- dt[train,]
y.train <- y[train]
x.test <- dt[!train,]
y.test <- y[!train]

# fit some linear classifiers
svm.lin.low <- svm(y~., data = x.train
              , kernel = "linear"
              , cost = 0.05, scale = FALSE)

ygrid <- predict(svm.lin.low,xgrid)
plot(xgrid,col=as.character(ygrid),pch=20,cex=.2)

# add all the points
points(dt, col=y,pch=19)
# mark the support vectors (from the training set)
points(x.train[svm.lin.low$index,],pch=5,cex=2)

# low cost test errors
y.pred <- predict(svm.lin.low, x.test)
test.error.lin.low <- sum(1 * (y.pred == y.test))/length(y.test)
test.error.lin.low


# try with a higher cost
svm.lin.high <- svm(y~., data = x.train
              , kernel = "linear"
              , cost = 10, scale = FALSE)

ygrid <- predict(svm.lin.high,xgrid)
plot(xgrid,col=as.character(ygrid),pch=20,cex=.2)

# add all the points
points(dt, col=y,pch=19)
# mark the support vectors (from the training set)
points(x.train[svm.lin.high$index,],pch=5,cex=2)

# high cost test errors
y.pred <- predict(svm.lin.high, x.test)
test.error.lin.high <- sum(1 * (y.pred == y.test))/length(y.test)
test.error.lin.high

set.seed(12345)
tune.out <- tune(svm, y∼.
                 , data = x.train
                 , kernel="linear"
                 , ranges=list(
                   cost=c(0.05, 0.1, 1
                          , 10, 100, 1000)
                   )
                 )
summary(tune.out)

# optimised for cost
# try with a higher cost
svm.lin.opt <- svm(y~., data = x.train
              , kernel = "linear"
              , cost = 0.1, scale = FALSE)

ygrid <- predict(svm.lin.opt,xgrid)
plot(xgrid,col=as.character(ygrid),pch=20,cex=.2)

# add all the points
points(dt, col=y,pch=19)
# mark the support vectors (from the training set)
points(x.train[svm.lin.opt$index,],pch=5,cex=2)

# opt cost test errors
y.pred <- predict(svm.lin.opt, x.test)
test.error.lin.opt <- sum(1 * (y.pred == y.test))/length(y.test)
test.error.lin.opt
```

