ISLR 6 Model Selection
======================

```{r}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
library(MASS)
```


```{r}
summary(Hitters)
# remove rows with missing values
Hitters <- na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```


Best Subset Selection
=====================
```{r}
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
```
That gave us the default max model size of 8 vars.
We can increase it.

```{r}
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax = 19)
summary(regfit.full)
summary.regfit <- summary(regfit.full)
names(summary.regfit)
# see the R Squared increase monotonically as more variables are added.
summary.regfit$rsq

par(mfrow = c(1,2))
plot(summary.regfit$rss ,xlab="Number of Variables ",ylab="RSS", type= "b")

plot(summary.regfit$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.regfit$cp), summary.regfit$cp[which.min(summary.regfit$cp)]
       , col = "red", pch = 19)
par(mfrow = c(1,1))
# Number of variables in best model
which.min(summary.regfit$cp)
which.max(summary.regfit$adjr2)
which.min(summary.regfit$bic)
```

There is a plot method for regsubsets
```{r}
plot(regfit.full, scale = "Cp")
coef(regfit.full, which.min(summary.regfit$cp))
plot(regfit.full, scale = "bic")
coef(regfit.full, which.min(summary.regfit$bic))
plot(regfit.full, scale = "adjr2")
coef(regfit.full, which.max(summary.regfit$adjr2))
```

Forward and Backwards Stepwise Selection
==========================

```{r}
regfit.fwd <- regsubsets(Salary~., data=Hitters
                          , nvmax = 19, method = "forward")
summary(regfit.fwd)
summary.regfwd <- summary(regfit.fwd)
plot(summary.regfwd$cp, xlab = "number of vars", ylab = "Cp")
points(which.min(summary.regfwd$cp), summary.regfwd$cp[which.min(summary.regfwd$cp)]
       , col = "red", pch = 19)
which.min(summary.regfwd$cp)
plot(regfit.fwd, scale = "Cp")
coef(regfit.fwd, which.min(summary.regfwd$cp))
```

```{r}
regfit.bwd <- regsubsets(Salary~., data=Hitters
                          , nvmax = 19, method = "backward")
summary(regfit.bwd)
summary.regbwd <- summary(regfit.bwd)
plot(summary.regbwd$cp, xlab = "number of vars", ylab = "Cp")
points(which.min(summary.regbwd$cp), summary.regbwd$cp[which.min(summary.regbwd$cp)]
       , col = "red", pch = 19)
which.min(summary.regbwd$cp)
plot(regfit.bwd, scale = "Cp")
coef(regfit.bwd, which.min(summary.regbwd$cp))
```

Different models are selected by the different approaches:

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

Validation set and cross validation to select models
-----------------------------------
```{r}
set.seed(121)
# what about this: 
# train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
train <- sample(dim(Hitters)[1], 180, replace = FALSE)
# create the training set
regfit.fwd <- regsubsets(Salary~., data=Hitters[train,]
                          , nvmax = 19, method = "forward")

# object to hold the MSE values
val.errors <- rep(NA,19)
# matrix for calculating the predictions
x.test <- model.matrix(Salary~., data = Hitters[-train,])
# loop through the best models sizes 1 - 19
for (i in 1:19) {
  # gather the coefs
  coefi <- coef(regfit.fwd, id = i)
  # calculate the preds
  pred <- x.test[,names(coefi)]%*%coefi
  # calcualte the errors
  val.errors[i] <- mean((Hitters$Salary[-train]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab="RMSE", ylim=c(280,350)
     , pch = 19, col = "blue", type = "b")
```


These results indicate the best model has 8 variables. It is necessary to refit the best subset with forward selection on the whole dataset and pick the 8 member model, which may be different from that picked of the training set.

```{r}
regfit.best <- regsubsets(Salary~.,data = Hitters, nvmax = 19)
coef(regfit.best, 8)
```

Prediction function for regsubsets
```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object, id = id)
  mat[,names(coefi)]%*%coefi
}
```

Setting up cross validation manually
------------------------------------
```{r}
set.seed(11)
folds <- sample(rep(1:10, length = nrow(Hitters)))
folds
# this way ensures a very even split
table(folds)

# matrix of 10 rows (for the folds) by 19 columns (for the predictors)
cv.errors <- matrix(NA, 10, 19)
# loop over the folds
for (k in 1:10) {
  best.fit <- regsubsets(Salary~., data = Hitters[folds!=k,]
                         ,nvmax = 19, method = "forward")
  # loop over the best subsets
  for (i in 1:19) {
    # this is using the predict function above
    pred <- predict(best.fit, Hitters[folds==k,], id = i)
    cv.errors[k,i] <- mean((Hitters$Salary[folds==k]-pred)^2)
  }
}
# average of the k-fold cv errors on columns
rmse.cv <- sqrt(apply(cv.errors, 2, mean))
rmse.cv
plot(rmse.cv, pch=19, type="b")
points(which.min(rmse.cv), rmse.cv[which.min(rmse.cv)], col = "red", pch = 19)
```

These results favoured a model with 12 members. In fact the valdiation set results were very variable and depended a lot on the set.seed parameter.

```{r}
coef(regfit.best, 12)
```

Lasso and Ridge
===============
First create the data arguments for glmnet
```{r}
library(glmnet)
x <- model.matrix(Salary~.-1, data = Hitters)
y <- Hitters$Salary
```

Setting up for ridge (alpha = 0)
```{r}
grid=10^seq(10,-2, length =100)
# default lambda is selected or we can provide a vector of values
fit.ridge <- glmnet(x,y,alpha=0, lambda = grid)
fit.ridge$lambda[1]
round(coef(fit.ridge)[,1],4)
fit.ridge$lambda[100]
round(coef(fit.ridge)[, 100],4)
plot(fit.ridge, xvar = "lambda")
# cv.glmnet runs a cv routine using the same parameters as glmnet()
cv.ridge <- cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

Can use the predict function to find the coefficients for a new values of $\lambda$

```{r}
round(predict(fit.ridge, s=50
              , type = "coefficients")[1:20,],4)
```

Now split a validation set to estimate the test error.

```{r}
set.seed(1231)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test <- !(train)
y.test <- y[test]
# train a ridge model
ridge.train <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# predict with lambda = 4
ridge.pred <- predict(ridge.train, s=4, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Note that a model with only an intercept would predict the mean of y values.

```{r}
mean((mean(y[train])-y.test)^2)
```

A model with a huge lambda should give the same results

```{r}
ridge.pred <- predict(ridge.train, s=1e10, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

If $\lambda$ is 0 then it's the same as a least squares fit.

```{r}
ridge.pred <- predict(ridge.train, s=0, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Now use glmnet cross validation to select the best value for $\lambda$

```{r}
set.seed(1001)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
# MSE associated with the best value of lambda
ridge.pred <- predict(ridge.train, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Use the model with all the data and predict using this value of $\lambda$

```{r}
predict(fit.ridge, s=bestlam, type = "coefficients")
```


Now a lasso (alpha = 1)

```{r}
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
```

Use the training and validation sets to have a look at the RMSE for all the lambda values collected in the model fit

```{r}
lasso.tr <- glmnet(x[train,], y[train])
pred.lasso <- predict(lasso.tr, newx = x[test,])
rmse <- sqrt(apply((y.test-pred.lasso)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "log(lambda)")
points(log(lasso.tr$lambda)[which.min(rmse)], rmse[which.min(rmse)], col = "red", pch = 19)
```

Now use the earlier validation sets to find the best $\lambda$
```{r}
lasso.train <- cv.glmnet(x[train,], y[train])
plot(lasso.train)
bestlam <- lasso.train$lambda.min
lasso.pred <- predict(lasso.train, s=bestlam, newx = x[test,])
mean((lasso.pred-y.test)^2)
bestlam
```

```{r}
# now get the details for the full set model with this value of lambda
lasso.coef <- predict(fit.lasso
                      ,type="coefficients",s=bestlam)[1:20,]
lasso.coef[lasso.coef != 0]
```

PCR and PLS
===========

PCR
--------

Fitting with validation = CV uses a built in cross validation and finds the best number of components.

```{r}
library(pls)
set.seed(2)
fit.pcr <- pcr(Salary~., data = Hitters, scale = TRUE, validation = "CV")
summary(fit.pcr)
validationplot(fit.pcr, val.type = "RMSEP")
```

can also try validation set method

```{r}
pcr.train <- pcr(Salary~., data = Hitters[train,], scale = TRUE, validation = "CV")
summary(pcr.train)
validationplot(pcr.train, val.type = "RMSEP")
```

and predict of the observed best model. In our case 5 comes second to 16 but there's no point in going as high as 16 as that's almost the entire model.

```{r}
pred.pcr <- predict(pcr.train, Hitters[test,], ncomp = 5)
mean((pred.pcr-y.test)^2)
```

Then refit the model with the selected number of components

```{r}
fit.pcr <- pcr(Salary~., data = Hitters, scale = TRUE, ncomp = 5)
summary(fit.pcr)
```

PLS
--------

Fitting with validation = CV uses a built in cross validation and finds the best number of components.

```{r}
set.seed(333)
pls.train <- plsr(Salary~., data = Hitters[train,], scale = TRUE, validation = "CV")
summary(pls.train)
validationplot(pls.train, val.type = "RMSEP")
```

Lowest adj error is for 2 components in this case. Can 

```{r}
pred.pls <- predict(pls.train, Hitters[test,], ncomp = 2)
mean((pred.pls-y.test)^2)
```

Then fit the 2 component model on all the data

```{r}
fit.pls <- plsr(Salary~., data = Hitters, scale = TRUE, ncomp = 2)
summary(fit.pls)
```

PLS can explain more of the response variance in fewer compononts because it the process tries to maximise variance explained in both predictors and response. PCA only does this in the predictors.

Applied
========

```{r}
set.seed(1001)
X <- rnorm(100)
noise <- rnorm(100)
y <- 5 + 10 * X + 0.5 * X^2 + (-2) * X^3 + noise
dat.frm <- data.frame(y = y, X = X)
```

Best subset selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11)

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Repeat for forward selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11, method = "forward")

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Repeat for backward selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11, method = "backward")

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Now a lasso glmnet(alpha = 1)

```{r}
x.lasso <- model.matrix(dat.frm$y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10))
fit.lasso <- glmnet(x.lasso,dat.frm$y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
set.seed(102)
fit.lasso.cv <- cv.glmnet(x.lasso,dat.frm$y)
plot(fit.lasso.cv)
bestlam <- fit.lasso.cv$lambda.min
bestlam

predict(fit.lasso, s = bestlam, type = "coefficients")[1:11, ]
```

A different data set
```{r}
dat.frm$y <- 5 + 7 * X^7

```

```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11)

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

and lasso
```{r}
fit.lasso <- glmnet(x.lasso,dat.frm$y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
set.seed(102)
fit.lasso.cv <- cv.glmnet(x.lasso,dat.frm$y)
plot(fit.lasso.cv)
bestlam <- fit.lasso.cv$lambda.min
bestlam

predict(fit.lasso, s = bestlam, type = "coefficients")[1:11, ]
```

```{r}
# Question 9
set.seed(1001001)
summary(College)
train <- sample(dim(College)[1], dim(College)[1] * 0.8, replace = FALSE)
  
# lm
fit.lm <- lm(Apps~., data = College[train,])
pred.lm <- predict(fit.lm, newdata = College[-train,])
mean((pred.lm - College$Apps[-train])^2) #MSE
sqrt(mean((pred.lm - College$Apps[-train])^2)) #RMSE

# First create the data arguments for glmnet
# really it makes sense to scale the data, but that's not what we did in the question
College.sc <- as.data.frame(scale(College[,-c(1,2)]))
College.sc$Private <- College$Private
College.sc$Apps <- College$Apps

x <- model.matrix(Apps~.-1, data = College.sc)
y <- College.sc$Apps

# alpha = 0 is a ridge regression
set.seed(1001001)
cv.out.ridge <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out.ridge)
bestlam.ridge <- cv.out.ridge$lambda.min
bestlam.ridge
selam.ridge <- cv.out.ridge$lambda.1se
selam.ridge

# Setting up for ridge (alpha = 0)
fit.ridge <- glmnet(x[train,],y[train],alpha=0)
plot(fit.ridge, xvar = "lambda")

# MSE associated with the best value of lambda
ridge.pred.best <- predict(fit.ridge, s=bestlam.ridge, newx = x[-train,])
mean((ridge.pred.best-y[-train])^2) #MSE
sqrt(mean((ridge.pred.best-y[-train])^2)) #RMSE
round(predict(fit.ridge, s=bestlam.ridge
              , type = "coefficients"),4)

# MSE associated with the good enough value of lambda
ridge.pred.se <- predict(fit.ridge, s=selam.ridge, newx = x[-train,])
mean((ridge.pred.se-y[-train])^2) #MSE
sqrt(mean((ridge.pred.se-y[-train])^2)) #RMSE
round(predict(fit.ridge, s=selam.ridge
              , type = "coefficients"),4)

# get the MSE/RMSE for the full data set on this value of lambda
fit.ridge.full <- glmnet(x,y,alpha=0, lambda = bestlam.ridge)
ridge.pred.full <- predict(fit.ridge.full, x)
mean((ridge.pred.full-y)^2) #MSE
sqrt(mean((ridge.pred.full-y)^2)) #RMSE

# Now a lasso (alpha = 1)
cv.out.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out.lasso)
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
selam.lasso <- cv.out.lasso$lambda.1se
selam.lasso

fit.lasso <- glmnet(x[train,],y[train],alpha=1)
plot(fit.lasso, xvar = "lambda")

# MSE associated with the best value of lambda
lasso.pred.best <- predict(fit.lasso, s=bestlam.lasso, newx = x[-train,])
mean((lasso.pred.best-y[-train])^2) #MSE
sqrt(mean((lasso.pred.best-y[-train])^2)) #RMSE
round(predict(fit.lasso, s=bestlam.lasso
              , type = "coefficients"),4)

# MSE associated with the good enough value of lambda
lasso.pred.se <- predict(fit.lasso, s=selam.lasso, newx = x[-train,])
mean((lasso.pred.se-y[-train])^2) #MSE
sqrt(mean((lasso.pred.se-y[-train])^2)) #RMSE
round(predict(fit.lasso, s=selam.lasso
              , type = "coefficients"),4)

# get the MSE/RMSE for the full data set on this value of lambda
fit.lasso.full <- glmnet(x,y,alpha=1, lambda = bestlam.lasso)
lasso.pred.full <- predict(fit.lasso.full, x)
mean((lasso.pred.full-y)^2) #MSE
sqrt(mean((lasso.pred.full-y)^2)) #RMSE

# Now with PCR
# Fitting with validation = CV 
# uses a built in cross validation 
# and finds the best number of components.
set.seed(101)
fit.pcr <- pcr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
summary(fit.pcr)
validationplot(fit.pcr, val.type = "MSEP")

# from the results could take a model with 2, 5 or 9 comps
pred.pcr2 <- predict(fit.pcr, College[-train,], ncomp = 2)
pred.pcr5 <- predict(fit.pcr, College[-train,], ncomp = 5)
pred.pcr9 <- predict(fit.pcr, College[-train,], ncomp = 9)

mean((pred.pcr2-College$Apps[-train])^2)
sqrt(mean((pred.pcr2-College$Apps[-train])^2))

mean((pred.pcr5-College$Apps[-train])^2)
sqrt(mean((pred.pcr5-College$Apps[-train])^2))

mean((pred.pcr9-College$Apps[-train])^2)
sqrt(mean((pred.pcr9-College$Apps[-train])^2))

# 9 is the best and in range of other approaches. 
# Refit the model with 9 comps on all the data
fit.pcr.full <- pcr(Apps~., data = College, nrcomp = 9)
summary(fit.pcr.full)
pred.pcr.full <- predict(fit.pcr.full, College)
mean((pred.pcr.full-College$Apps)^2)
sqrt(mean((pred.pcr.full-College$Apps)^2))

# Now PLS
set.seed(101)
fit.pls <- plsr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
summary(fit.pls)
validationplot(fit.pls, val.type = "MSEP")

# best model 6
pred.pls6 <- predict(fit.pls, College[-train,], ncomp = 6)
mean((pred.pls6-College$Apps[-train])^2)
sqrt(mean((pred.pls6-College$Apps[-train])^2))

# refit model 6 on whole set
fit.pls.full <- plsr(Apps~., data = College, ncomp = 6)
pred.pls.full <- predict(fit.pls.full, College)
mean((pred.pls.full-College$Apps)^2)
sqrt(mean((pred.pls.full-College$Apps)^2))

# To compare the results obtained above, 
# we have to compute the test R2R2 for all models.

test.avg <- mean(College$Apps[-train])
lm.r2 <- 1 - mean((pred.lm - College$Apps[-train])^2) / mean((test.avg - College$Apps[-train])^2)
ridge.r2 <- 1 - mean((ridge.pred.best - College$Apps[-train])^2) / mean((test.avg - College$Apps[-train])^2)
lasso.r2 <- 1 - mean((lasso.pred.best - College$Apps[-train])^2) / mean((test.avg - College$Apps[-train])^2)
pcr.r2 <- 1 - mean((pred.pcr9 - College$Apps[-train])^2) / mean((test.avg - College$Apps[-train])^2)
pls.r2 <- 1 - mean((pred.pls6 - College$Apps[-train])^2) / mean((test.avg - College$Apps[-train])^2)

lm.r2
ridge.r2
lasso.r2
pcr.r2
pls.r2
```

```{r}
# Question 10
set.seed(101101)

# data simulation, 20 predictors, some zero coefs
X <- matrix(NA, 1000, 20)
colnames(X) <- paste0("X",1:20)
for(i in 1:20) {
  X[,i] <- rnorm(1000, mean = i, sd = sample(1:20)/sample(1:20))  
}
beta <- c(1,2,3,4,5,1/6,1/7,1/8,1/9,-1,-1.1,-1.2,-1.3,0,0,0,0,0,0,0)
eps <- rnorm(1000, sd = 0.1)
y <- X %*% beta + eps 

train <- sample(1000, 900)
test <- (1:1000)[-train]

data.train <- data.frame(y = y, X)[train,]
train.mat <- model.matrix(y ~ ., data = data.train, nvmax = 20)

data.test <- data.frame(y = y, X)[test,]
test.mat <- model.matrix(y ~ ., data = data.test, nvmax = 20)

# fit best subset
fit.bss <- regsubsets(y~., data=data.train, nvmax = 20)
fit.bss.smry <- summary(fit.bss)
fit.bss.smry

oldpar <- par()
par(mfrow = c(1,4))
plot(fit.bss.smry$rss ,xlab="Number of Variables ",ylab="RSS (Training Error)", type= "b")
plot(fit.bss.smry$cp ,xlab="Number of Variables ",ylab="Cp", type= "b")
points(which.min(fit.bss.smry$cp), fit.bss.smry$cp[which.min(fit.bss.smry$cp)], col = "red", pch = 19)
plot(fit.bss.smry$adjr2 ,xlab="Number of Variables ",ylab="Adjusted R2", type= "b")
points(which.max(fit.bss.smry$adjr2), fit.bss.smry$adjr2[which.max(fit.bss.smry$adjr2)], col = "red", pch = 19)
plot(fit.bss.smry$bic ,xlab="Number of Variables ",ylab="bic", type= "b")
points(which.min(fit.bss.smry$bic), fit.bss.smry$bic[which.min(fit.bss.smry$bic)], col = "red", pch = 19)
par(oldpar)

# report the training MSE for each model size
val.errors <- rep(NA, 20)
for (i in 1:20) {
  coefi <- coef(fit.bss, id = i)
  pred <- train.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((pred - y[train])^2)
}
plot(val.errors, xlab = "Number of predictors"
     , ylab = "Training MSE", pch = 19, type = "b")

which.min(val.errors)

# report the test MSE for each model size
val.errors <- rep(NA, 20)
for (i in 1:20) {
  coefi <- coef(fit.bss, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((pred - y[test])^2)
}
plot(val.errors, xlab = "Number of predictors"
     , ylab = "Test MSE", pch = 19, type = "b")

which.min(val.errors)
coef(fit.bss, id = which.min(val.errors))

# plot root squared error of coef estimates
colnum <- function(c) {
  as.integer(sub("X","", c))
}
val.errors <- rep(NA, 20)
for (i in 1:20) {
  coefi <- coef(fit.bss, id = i)
  print(coefi)
  coln <- names(coefi)[names(coefi) != "(Intercept)"]
  colnums <- sapply(coln, colnum)
  betas <- c(0, beta[colnums])
  val.errors[i] <- sqrt(sum((coefi - betas)^2))
}
plot(val.errors, xlab = "Number of predictors"
     , ylab = "Error between estimated and true coeffs", pch = 19, type = "b")
which.min(val.errors)
```

```{r}
# Question 11
set.seed(12321)

k <- 5
folds <- sample(1:k, dim(Boston)[1], replace = TRUE)
coefs <- matrix(NA, 5 * 13, 14)
val.errors <- matrix(NA, 5, 13)

for (i in 1:k) {
  Boston.train <- Boston[folds != i,]
  Boston.test <- Boston[folds == i,]
  fit.fwd <- regsubsets(crim~., data=Boston.train, nvmax = 13, method = "forward")
  test.mat <- model.matrix(crim~., data = Boston.test)
    for (j in 1:13) {
      coefi <- coef(fit.fwd, id = j)
      coefs[j + (i-1) * 13, 1:(j+1)] <- coefi
      pred <- test.mat[, names(coefi)] %*% coefi
      val.errors[i,j] <- mean((pred - Boston.test$crim)^2)
    }    
}

coefs
cv.errors <- colMeans(val.errors)
cv.errors
plot(cv.errors, type = "b", xlab = "Number of variables", ylab = "CV error")

# over the cross validation it appears that all the predictors are required

# now for lasso
# I'll use fold 1 as a test set to determine the OOS error rate
# the lambda selection is done by cross validation inside the glmnet library
bestlams <- rep(NA, k)
MSEs <- rep(NA, k)
coef.table <- matrix(NA, 14, 5)
for (i in 1:k) {
  set.seed(1001)
  x <- model.matrix(crim~., data = Boston[folds != i,])[, -1]
  x.test <- model.matrix(crim~., data = Boston[folds == i,])[, -1]
  y <- Boston$crim[folds != i]
  y.test <- Boston$crim[folds == i]
  
  cv.Boston.lasso <- cv.glmnet(x, y, alpha = 1)
  plot(cv.Boston.lasso)
  bestlams[i] <- cv.Boston.lasso$lambda.min
  
  fit.Boston.lasso <- glmnet(x,y,alpha=1)
  plot(fit.lasso, xvar = "lambda")
  
  # OOB - using the held back fold, MSE associated with the best value of lambda
  pred.Boston.lasso <- predict(fit.Boston.lasso, s=bestlams[i], newx = x.test)
  MSEs[i] <- mean((pred.Boston.lasso-y.test)^2) #MSE
  coef.table[1:14,i] <- as.numeric(round(predict(fit.Boston.lasso, s=bestlams[i]
                , type = "coefficients"),4))
}
coef.table[,which.min(MSEs)]

# the lowest MSE was from fold 2. The model has a lambda of approx 0.03 and includes all the features
```

