ISLR 6 Model Selection
======================

```{r}
library(ISLR)
summary(Hitters)
# remove rows with missing values
Hitters <- na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```


Best Subset Selection
=====================
```{r}
library(leaps)
regfit.full <- regsubsets(Salary~., data=Hitters)
summary(regfit.full)
```
That gave us the default max model size of 8 vars.
We can increase it.

```{r}
regfit.full <- regsubsets(Salary~., data=Hitters, nvmax = 19)
summary(regfit.full)
summary.regfit <- summary(regfit.full)
names(summary.regfit)
# see the R Squared increase monotonically as more variables are added.
summary.regfit$rsq

par(mfrow = c(1,2))
plot(summary.regfit$rss ,xlab="Number of Variables ",ylab="RSS", type= "b")

plot(summary.regfit$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.regfit$cp), summary.regfit$cp[which.min(summary.regfit$cp)]
       , col = "red", pch = 19)
par(mfrow = c(1,1))
# Number of variables in best model
which.min(summary.regfit$cp)
which.max(summary.regfit$adjr2)
which.min(summary.regfit$bic)
```

There is a plot method for regsubsets
```{r}
plot(regfit.full, scale = "Cp")
coef(regfit.full, which.min(summary.regfit$cp))
plot(regfit.full, scale = "bic")
coef(regfit.full, which.min(summary.regfit$bic))
plot(regfit.full, scale = "adjr2")
coef(regfit.full, which.max(summary.regfit$adjr2))
```

Forward and Backwards Stepwise Selection
==========================

```{r}
regfit.fwd <- regsubsets(Salary~., data=Hitters
                          , nvmax = 19, method = "forward")
summary(regfit.fwd)
summary.regfwd <- summary(regfit.fwd)
plot(summary.regfwd$cp, xlab = "number of vars", ylab = "Cp")
points(which.min(summary.regfwd$cp), summary.regfwd$cp[which.min(summary.regfwd$cp)]
       , col = "red", pch = 19)
which.min(summary.regfwd$cp)
plot(regfit.fwd, scale = "Cp")
coef(regfit.fwd, which.min(summary.regfwd$cp))
```

```{r}
regfit.bwd <- regsubsets(Salary~., data=Hitters
                          , nvmax = 19, method = "backward")
summary(regfit.bwd)
summary.regbwd <- summary(regfit.bwd)
plot(summary.regbwd$cp, xlab = "number of vars", ylab = "Cp")
points(which.min(summary.regbwd$cp), summary.regbwd$cp[which.min(summary.regbwd$cp)]
       , col = "red", pch = 19)
which.min(summary.regbwd$cp)
plot(regfit.bwd, scale = "Cp")
coef(regfit.bwd, which.min(summary.regbwd$cp))
```

Different models are selected by the different approaches:

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

Validation set and cross validation to select models
-----------------------------------
```{r}
set.seed(121)
# what about this: 
# train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
train <- sample(dim(Hitters)[1], 180, replace = FALSE)
# create the training set
regfit.fwd <- regsubsets(Salary~., data=Hitters[train,]
                          , nvmax = 19, method = "forward")

# object to hold the MSE values
val.errors <- rep(NA,19)
# matrix for calculating the predictions
x.test <- model.matrix(Salary~., data = Hitters[-train,])
# loop through the best models sizes 1 - 19
for (i in 1:19) {
  # gather the coefs
  coefi <- coef(regfit.fwd, id = i)
  # calculate the preds
  pred <- x.test[,names(coefi)]%*%coefi
  # calcualte the errors
  val.errors[i] <- mean((Hitters$Salary[-train]-pred)^2)
}
val.errors
plot(sqrt(val.errors), ylab="RMSE", ylim=c(280,350)
     , pch = 19, col = "blue", type = "b")
```


These results indicate the best model has 8 variables. It is necessary to refit the best subset with forward selection on the whole dataset and pick the 8 member model, which may be different from that picked of the training set.

```{r}
regfit.best <- regsubsets(Salary~.,data = Hitters, nvmax = 19)
coef(regfit.best, 8)
```

Prediction function for regsubsets
```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object, id = id)
  mat[,names(coefi)]%*%coefi
}
```

Setting up cross validation manually
------------------------------------
```{r}
set.seed(11)
folds <- sample(rep(1:10, length = nrow(Hitters)))
folds
# this way ensures a very even split
table(folds)

# matrix of 10 rows (for the folds) by 19 columns (for the predictors)
cv.errors <- matrix(NA, 10, 19)
# loop over the folds
for (k in 1:10) {
  best.fit <- regsubsets(Salary~., data = Hitters[folds!=k,]
                         ,nvmax = 19, method = "forward")
  # loop over the best subsets
  for (i in 1:19) {
    # this is using the predict function above
    pred <- predict(best.fit, Hitters[folds==k,], id = i)
    cv.errors[k,i] <- mean((Hitters$Salary[folds==k]-pred)^2)
  }
}
# average of the k-fold cv errors on columns
rmse.cv <- sqrt(apply(cv.errors, 2, mean))
rmse.cv
plot(rmse.cv, pch=19, type="b")
points(which.min(rmse.cv), rmse.cv[which.min(rmse.cv)], col = "red", pch = 19)
```

These results favoured a model with 12 members. In fact the valdiation set results were very variable and depended a lot on the set.seed parameter.

```{r}
coef(regfit.best, 12)
```

Lasso and Ridge
===============
First create the data arguments for glmnet
```{r}
library(glmnet)
x <- model.matrix(Salary~.-1, data = Hitters)
y <- Hitters$Salary
```

Setting up for ridge (alpha = 0)
```{r}
grid=10^seq(10,-2, length =100)
# default lambda is selected or we can provide a vector of values
fit.ridge <- glmnet(x,y,alpha=0, lambda = grid)
fit.ridge$lambda[1]
round(coef(fit.ridge)[,1],4)
fit.ridge$lambda[100]
round(coef(fit.ridge)[, 100],4)
plot(fit.ridge, xvar = "lambda")
# cv.glmnet runs a cv routine using the same parameters as glmnet()
cv.ridge <- cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

Can use the predict function to find the coefficients for a new values of $\lambda$

```{r}
round(predict(fit.ridge, s=50
              , type = "coefficients")[1:20,],4)
```

Now split a validation set to estimate the test error.

```{r}
set.seed(1231)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test <- !(train)
y.test <- y[test]
# train a ridge model
ridge.train <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# predict with lambda = 4
ridge.pred <- predict(ridge.train, s=4, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Note that a model with only an intercept would predict the mean of y values.

```{r}
mean((mean(y[train])-y.test)^2)
```

A model with a huge lambda should give the same results

```{r}
ridge.pred <- predict(ridge.train, s=1e10, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

If $\lambda$ is 0 then it's the same as a least squares fit.

```{r}
ridge.pred <- predict(ridge.train, s=0, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Now use glmnet cross validation to select the best value for $\lambda$

```{r}
set.seed(1001)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
# MSE associated with the best value of lambda
ridge.pred <- predict(ridge.train, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

Use the model with all the data and predict using this value of $\lambda$

```{r}
predict(fit.ridge, s=bestlam, type = "coefficients")
```


Now a lasso (alpha = 1)

```{r}
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
```

Use the training and validation sets to have a look at the RMSE for all the lambda values collected in the model fit

```{r}
lasso.tr <- glmnet(x[train,], y[train])
pred.lasso <- predict(lasso.tr, newx = x[test,])
rmse <- sqrt(apply((y.test-pred.lasso)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "log(lambda)")
points(log(lasso.tr$lambda)[which.min(rmse)], rmse[which.min(rmse)], col = "red", pch = 19)
```

Now use the earlier validation sets to find the best $\lambda$
```{r}
lasso.train <- cv.glmnet(x[train,], y[train])
plot(lasso.train)
bestlam <- lasso.train$lambda.min
lasso.pred <- predict(lasso.train, s=bestlam, newx = x[test,])
mean((lasso.pred-y.test)^2)
bestlam
```

```{r}
# now get the details for the full set model with this value of lambda
lasso.coef <- predict(fit.lasso
                      ,type="coefficients",s=bestlam)[1:20,]
lasso.coef[lasso.coef != 0]
```

PCR and PLS
===========

PCR
--------

Fitting with validation = CV uses a built in cross validation and finds the best number of components.

```{r}
library(pls)
set.seed(2)
fit.pcr <- pcr(Salary~., data = Hitters, scale = TRUE, validation = "CV")
summary(fit.pcr)
validationplot(fit.pcr, val.type = "RMSEP")
```

can also try validation set method

```{r}
pcr.train <- pcr(Salary~., data = Hitters[train,], scale = TRUE, validation = "CV")
summary(pcr.train)
validationplot(pcr.train, val.type = "RMSEP")
```

and predict of the observed best model. In our case 5 comes second to 16 but there's no point in going as high as 16 as that's almost the entire model.

```{r}
pred.pcr <- predict(pcr.train, Hitters[test,], ncomp = 5)
mean((pred.pcr-y.test)^2)
```

Then refit the model with the selected number of components

```{r}
fit.pcr <- pcr(Salary~., data = Hitters, scale = TRUE, ncomp = 5)
summary(fit.pcr)
```

PLS
--------

Fitting with validation = CV uses a built in cross validation and finds the best number of components.

```{r}
set.seed(333)
pls.train <- plsr(Salary~., data = Hitters[train,], scale = TRUE, validation = "CV")
summary(pls.train)
validationplot(pls.train, val.type = "RMSEP")
```

Lowest adj error is for 2 components in this case. Can 

```{r}
pred.pls <- predict(pls.train, Hitters[test,], ncomp = 2)
mean((pred.pls-y.test)^2)
```

Then fit the 2 component model on all the data

```{r}
fit.pls <- plsr(Salary~., data = Hitters, scale = TRUE, ncomp = 2)
summary(fit.pls)
```

PLS can explain more of the response variance in fewer compononts because it the process tries to maximise variance explained in both predictors and response. PCA only does this in the predictors.

Applied
========

```{r}
set.seed(1001)
X <- rnorm(100)
noise <- rnorm(100)
y <- 5 + 10 * X + 0.5 * X^2 + (-2) * X^3 + noise
dat.frm <- data.frame(y = y, X = X)
```

Best subset selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11)

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Repeat for forward selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11, method = "forward")

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Repeat for backward selection
```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11, method = "backward")

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

Now a lasso glmnet(alpha = 1)

```{r}
x.lasso <- model.matrix(dat.frm$y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10))
fit.lasso <- glmnet(x.lasso,dat.frm$y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
set.seed(102)
fit.lasso.cv <- cv.glmnet(x.lasso,dat.frm$y)
plot(fit.lasso.cv)
bestlam <- fit.lasso.cv$lambda.min
bestlam

predict(fit.lasso, s = bestlam, type = "coefficients")[1:11, ]
```

A different data set
```{r}
dat.frm$y <- 5 + 7 * X^7

```

```{r}
reg.best <- regsubsets(y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5)+I(X^6)+I(X^7)+I(X^8)+I(X^9)+I(X^10)
                       , data = dat.frm, nvmax = 11)

summary.reg.best <- summary(reg.best)
summary.reg.best

# Number of variables in best model
which.min(summary.reg.best$cp)
which.max(summary.reg.best$adjr2)
which.min(summary.reg.best$bic)

par(mfrow = c(1,3))
plot(summary.reg.best$cp, xlab = "number of vars", ylab = "Cp", type = "b")
points(which.min(summary.reg.best$cp), summary.reg.best$cp[which.min(summary.reg.best$cp)]
       , col = "red", pch = 19)

plot(summary.reg.best$bic, xlab = "number of vars", ylab = "bic", type = "b")
points(which.min(summary.reg.best$bic), summary.reg.best$bic[which.min(summary.reg.best$bic)]
       , col = "red", pch = 19)

plot(summary.reg.best$adjr2, xlab = "number of vars", ylab = "adjr2", type = "b")
points(which.max(summary.reg.best$adjr2), summary.reg.best$adjr2[which.max(summary.reg.best$adjr2)]
       , col = "red", pch = 19)

par(mfrow = c(1,1))

plot(reg.best, scale = "Cp")
coef(reg.best, which.min(summary.reg.best$cp))
plot(reg.best, scale = "bic")
coef(reg.best, which.min(summary.reg.best$bic))
plot(reg.best, scale = "adjr2")
coef(reg.best, which.max(summary.reg.best$adjr2))
```

and lasso
```{r}
fit.lasso <- glmnet(x.lasso,dat.frm$y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
set.seed(102)
fit.lasso.cv <- cv.glmnet(x.lasso,dat.frm$y)
plot(fit.lasso.cv)
bestlam <- fit.lasso.cv$lambda.min
bestlam

predict(fit.lasso, s = bestlam, type = "coefficients")[1:11, ]
```
