# Trees
```{r prologue, results='hide', echo=FALSE}
knitr::opts_chunk$set(warning = FALSE
                      , message = FALSE
                      , echo = FALSE
                      )
```

We're going to have a look at trees, using the Carseats data
```{r setup}
require(ISLR)
require(tree)
require(lattice)
attach(Carseats)

histogram(Sales)
High <- ifelse(Sales <= 8, "No", "Yes")
Carseats <- data.frame(Carseats, High)
```

Want to fit a classification tree and need to exclude Sales as the response variable was created from Sales.

```{r model1}
tree.carseats <- tree(High~.-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

For a detailed version of the tree, it can be printed:
```{r model1.detail}
tree.carseats
```

Split into training and test
```{r train.val}
set.seed(1011)
train <- sample(1:nrow(Carseats), 250)
tree.carseats <- tree(High~.-Sales, data = Carseats, subset = train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)

tree.pred <- predict(tree.carseats
                     , Carseats[-train,]
                     , type="class")

conf.mat <- with(Carseats[-train,], table(tree.pred,High))
conf.mat
(conf.mat[1,1] + conf.mat[2,2])/sum(conf.mat)
```

Can use cross validation to recommend pruning:
```{r xval.prune}
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
best.prune <- which.min(cv.carseats$dev)
best.prune
```

And then apply the pruning function

```{r prune.tree}
prune.carseats <- prune.misclass(tree.carseats
                                 , best = cv.carseats$dev[best.prune])
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Final validation

```{r validate.pruned.tree}
tree.pred <- predict(prune.carseats
                     , Carseats[-train,]
                     , type="class")

conf.mat <- with(Carseats[-train,], table(tree.pred,High))
conf.mat
(conf.mat[1,1] + conf.mat[2,2])/sum(conf.mat)
```

This hasn't done brilliantly.

Trees can be used as building blocks in more complex models.

```{r randfor.setup}
require(randomForest)
require(MASS)
set.seed(101)
train <- sample(1:nrow(Boston), 300)
```

Will build a random forest with response medv

```{r randfor.one}
rf.boston <- randomForest(medv~., data = Boston, subset = train)
rf.boston
```

Will now look at analysing error rates
```{r randfor.xval}
oob.err <- double(13)
test.err <- double(13)
for (mtry in 1:13) {
  randfor.boston <- randomForest(medv~., data = Boston, subset = train, mtry = mtry, ntree = 400)
  
  oob.err[mtry] <- randfor.boston$mse[400]
  pred <- predict(randfor.boston, Boston[-train,])
  test.err[mtry] <- with(Boston[-train,],mean((medv-pred)^2))
  cat(mtry, " ")
}
matplot(1:mtry, cbind(test.err, oob.err)
        , pch = 19, col = c("red", "blue")
        , type = "b", ylab = "mean squared error")
legend("topright", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
```


Now for boosting

```{r boost.setup}
require(gbm)
boost.boston <- gbm(medv~., data = Boston[train,]
                    , distribution = "gaussian"
                    , n.trees = 10000
                    , shrinkage = 0.01
                    , interaction.depth = 4)
```

Can plot variable importance and the influence of specific variables:

```{r boost.analysis}
summary(boost.boston)
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

And use cross validation to find the best params
```{r boost.xval}
n.trees <- seq(100, 10000, 100)
predmat <- predict(boost.boston, Boston[-train,], n.trees = n.trees)
dim(predmat)
berr <- with(Boston[-train,], apply((predmat-medv)^2, 2, mean))

plot(n.trees, berr, pch = 19, ylab = "mean squared error")
abline(h = min(test.err), col = "red")
```

We can see this did better than the randfor

# Lab Regresion Trees

```{r}
library(MASS)
library(tree)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv∼.,Boston ,subset=train)
summary(tree.boston)
print(tree.boston)
```

The deviance is simply the sum of squared errors for the residuals.

3 Variables have been used.

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

Will pruning the tree improve accuracy?
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

CV chooses the most complicated tree. If we wanted to prune, can do this:

```{r}
prune.boston <- prune.tree(tree.boston, best = 4)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

Time to make some predictions on the test set:

```{r}
yhat.tree <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston$medv[-train]
plot(yhat.tree, boston.test)
abline(0,1)
mean((yhat.tree-boston.test)^2)
```

repeat with pruned tree

```{r}
yhat.prune <- predict(prune.boston, newdata = Boston[-train,])
plot(yhat.prune, boston.test)
abline(0,1)
mean((yhat.prune-boston.test)^2)
```

Test MSE for boston.tree is 25.05, therefore RMSE is approx 5. RMSE is on the same scale as the variable in question, medv which is measured in $1,000. SO this tree gives predictions within $5,000.

# Random Forest
```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, importance = TRUE)
```

How well does this perform?

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

The test MSE is half that of optimally pruned single tree.

Can adjust the number of trees:

```{r}
set.seed(1)
bag.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 13, ntree = 25, importance = TRUE)
```

How well does this perform?

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```


Can grow a random forest by adjusting mtry - number of variables used at each split:

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data = Boston, subset = train, mtry = 6, importance = TRUE)
```

How well does this perform?

```{r}
yhat.rf <- predict(rf.boston, newdata = Boston[-train,])
plot(yhat.rf, boston.test)
abline(0,1)
mean((yhat.rf-boston.test)^2)
```

Can view the importance of each variable:

```{r}
importance(rf.boston)
```

and plots of these measures:
```{r}
varImpPlot(rf.boston)
```

# Boosting
Let's get on with it

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data = Boston[train,]
                    , distribution = "gaussian"
                    , n.trees = 5000
                    , interaction.depth = 4)
summary(boost.boston)
```

Partial dependance plots are useful for visualising marginal effects after integrating out the other variables.

```{r}
par(mfrow = c(1,2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
par(mfrow = c(1,1))
```

Predicting off the boosted model:

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
plot(yhat.boost, boston.test)
abline(0,1)
mean((yhat.boost-boston.test)^2)
```

Try with a different $\lambda$ value

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data = Boston[train,]
                    , distribution = "gaussian"
                    , n.trees = 5000
                    , interaction.depth = 4
                    , shrinkage = 0.2
                    , verbose = FALSE)
summary(boost.boston)
```


```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
plot(yhat.boost, boston.test)
abline(0,1)
mean((yhat.boost-boston.test)^2)
```

# Exercises
## Question 2 - Comparing deviance, gini and cross entropy

Calcultating Gini index, cross entropy and classification error rate.

$$G = \sum_{k=1}^K{\hat p_{mk}}(1-{\hat p_{mk}})$$


$$D = \sum_{k=1}^K{\hat p_{mk}}{\log {\hat p_{mk}}}$$


$$E = 1 - \max_k({\hat p_{mk}})$$

```{r}
gini <- function(p) {
  v <- p * (1-p)
  sum(v)
}

class.error <- function(p) {
  v <- 1-max(p)
  v
}
  
cross.entropy <- function(p) {
  v <- p * log(p)
  -sum(v)
}

p <- matrix(nrow = 101, ncol = 2)
p[,1] <- seq(0, 1, 0.01)
p[,2] <- 1 - p[,1]

impurity.measures <- matrix(nrow = 101, ncol = 3)
impurity.measures[,1] <- apply(p, 1, class.error)
impurity.measures[,2] <- apply(p, 1, gini)
impurity.measures[,3] <- apply(p, 1, cross.entropy)

matplot(p[,1], impurity.measures, type = "l")
```

```{r, eval=FALSE}
# this takes a really long time
library(MASS)
library(randomForest)
set.seed(121)
train <- sample(1:nrow(Boston), nrow(Boston) * 0.8)

Boston.train <- Boston[train,]
Boston.test <- Boston[-train,]

p <- c(4, 7, 13)
nt <- 500
val.errors <- matrix(NA, nt, 3)

for (i in 1:nt) {
  for (j in 1:3) {
    rf.boston <- randomForest(medv~.
                              , data = Boston.train
                              , ntree = i, mtry = p[j])
    yhat.rf <- predict(rf.boston, newdata = Boston.test)
    val.errors[i,j] <- mean((yhat.rf-Boston.test$medv)^2)

  }
}

matplot(1:500, val.errors, type = "l")
```

```{r}
library(ISLR)
library(tree)

set.seed(12)
train <- sample(1:nrow(Carseats), nrow(Carseats) * 0.8)

tree.carseats=tree(Sales∼., data = Carseats, subset=train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

What's the test error rate:
  
```{r}
yhat.tree <- predict(tree.carseats, newdata = Carseats[-train,])
carseats.test <- Carseats$Sales[-train]
plot(yhat.tree, carseats.test)
abline(0,1)
mean((yhat.tree-carseats.test)^2)
sqrt(mean((yhat.tree-carseats.test)^2))
```

Will pruning the tree improve accuracy?

```{r}
set.seed(4)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(cv.carseats$size[tree.min], cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
```

In this case CV chooses a tree with 7 internal nodes.

```{r}
prune.carseats <- prune.tree(tree.carseats, best = 7)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Test error with pruned tree

```{r}
yhat.prune <- predict(prune.carseats, newdata = Carseats[-train,])
plot(yhat.prune, carseats.test)
abline(0,1)
mean((yhat.prune-carseats.test)^2)
sqrt(mean((yhat.prune-carseats.test)^2))
```

Use bagging

```{r}
library(randomForest)
set.seed(4)
bag.carseats <- randomForest(Sales~., data = Carseats
                            , subset = train
                            , mtry = 10
                            , importance = TRUE)
```

```{r}
yhat.bag <- predict(bag.carseats, newdata = Carseats[-train,])
plot(yhat.bag, carseats.test)
abline(0,1)
mean((yhat.bag-carseats.test)^2)
sqrt(mean((yhat.bag-carseats.test)^2))

importance(bag.carseats)
varImpPlot(bag.carseats)
```

now use RF

```{r}
Carseats.train <- Carseats[train,]
Carseats.test <- Carseats[-train,]
set.seed(4)
p <- c(13, 6, 3, 2)
nt <- 500
val.errors <- matrix(NA, nt, 3)


for (j in 1:3) {
  for (i in 1:nt) {
    rf.carseats <- randomForest(Sales~.
                              , data = Carseats.train
                              , ntree = i, mtry = p[j])
    yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
    val.errors[i,j] <-  mean((yhat.rf-Carseats.test$Sales)^2)
  }
  importance(rf.carseats)
  varImpPlot(rf.carseats)
}

matplot(1:500, val.errors, type = "l")
```

OJ Data - question 9
```{r}
library(ISLR)
library(tree)
set.seed(10011)
train <- sample(1:nrow(OJ), 800)

OJ.train <- OJ[train,]
OJ.test <- OJ[-train,]

OJ.tree <- tree(Purchase~., data = OJ)
summary(OJ.tree)
OJ.tree

plot(OJ.tree)
text(OJ.tree, pretty = 0)

pred.OJ.tree <- predict(OJ.tree, newdata = OJ.test, type = "class")
confmat <- table(pred.OJ.tree, OJ.test$Purchase)
confmat
misclass.err <- 1 - sum(confmat[row(confmat) == col(confmat)])/sum(confmat)

misclass.err
```

```{r}
set.seed(5554)
cv.OJ.tree <- cv.tree(OJ.tree, FUN = prune.misclass)

plot(cv.OJ.tree$size, cv.OJ.tree$dev, type = "b")
tree.min <- which.min(cv.OJ.tree$dev)
points(cv.OJ.tree$size[tree.min], cv.OJ.tree$dev[tree.min], col = "red", cex = 2, pch = 20)

```

```{r}
OJ.prune <- prune.tree(OJ.tree, best = 6)
plot(OJ.prune)
text(OJ.prune, pretty = 0)

pred.OJ.prune <- predict(OJ.prune, newdata = OJ.test, type = "class")
confmat <- table(pred.OJ.prune, OJ.test$Purchase)
confmat
misclass.err <- 1 - sum(confmat[row(confmat) == col(confmat)])/sum(confmat)
misclass.err
```

# Question 10 - Boosting
```{r}
library(gbm)
Hitters.clean <- Hitters[!is.na(Hitters$Salary),]
Hitters.clean$Salary <- log(Hitters.clean$Salary)

set.seed(7)
train <- sample(1:nrow(Hitters.clean), 200)

lambda.grid <- seq(0.001, 0.1, 0.001)
trn.errors <- rep(NA, 100)
test.errors <- rep(NA, 100)
ntrees <- 1000

for (i in 1:100) {
  gbm.hitters <- gbm(Salary~., data = Hitters.clean[train,]
                   , distribution = "gaussian"
                   , n.trees = ntrees
                   , shrinkage = lambda.grid[i])
  trn.errors[i] <- gbm.hitters$train.error[ntrees]
  yhat <- predict(gbm.hitters, Hitters.clean[-train,]
                  , n.trees = 1000)
  test.errors[i] <- mean((yhat - Hitters.clean$Salary[-train])^2)
}
plot(lambda.grid, trn.errors, type = "l")
plot(lambda.grid, test.errors, type = "l")

lambda.grid[which.min(test.errors[lambda.grid < 0.04])]
```

compare with LM

```{r}
hitters.lm <- lm(Salary~., Hitters.clean, subset = train)
pred.hitters.lm <- predict(hitters.lm, Hitters.clean[-train,])

plot(pred.hitters.lm, Hitters.clean$Salary[-train])
abline(0,1)

mean((pred.hitters.lm - Hitters.clean$Salary[-train])^2)
```

compare with lasso
```{r}
library(glmnet)
# create the model matrix
x <- model.matrix(Salary~., data = Hitters.clean)
y <- Hitters.clean$Salary
set.seed(71)
cv.hitters.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.hitters.lasso)

bestlam.lasso <- cv.hitters.lasso$lambda.min
bestlam.lasso

fit.lasso <- glmnet(x[train,],y[train],alpha=1)
plot(fit.lasso, xvar = "lambda")

# MSE associated with the best value of lambda
lasso.pred.best <- predict(fit.lasso, s=bestlam.lasso, newx = x[-train,])
mean((lasso.pred.best-y[-train])^2) #MSE

round(predict(fit.lasso, s=bestlam.lasso
              , type = "coefficients"),4)
```

bagging

```{r}
hitters.bag <- randomForest(Salary~.
                            , data = Hitters.clean
                            , subset = train
                            , mtry = 19)

pred.hitters.bag <- predict(hitters.bag, Hitters.clean[-train,])

plot(pred.hitters.bag, Hitters.clean$Salary[-train])
abline(0,1)

mean((pred.hitters.bag - Hitters.clean$Salary[-train])^2)
```

# Question 11
```{r}
library(ISLR)
library(gbm)
set.seed(1)
train <- 1:1000
Caravan$Purchase <- ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train <- Caravan[train, ]
Caravan.test <- Caravan[-train, ]

set.seed(1)
boost.caravan <- gbm(Purchase ~ ., data = Caravan.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.01)

summary(boost.caravan)
```

```{r}
probs.test <- predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
pred.test <- ifelse(probs.test > 0.2, 1, 0)
table(pred.test, Caravan.test$Purchase)
```

```{r}
logit.caravan <- glm(Purchase ~ ., data = Caravan.train, family = "binomial")
probs.test2 <- predict(logit.caravan, Caravan.test, type = "response")

pred.test2 <- ifelse(probs.test > 0.2, 1, 0)
table(pred.test2, Caravan.test$Purchase)
```

```{r}
library(class)
knn.pred=knn(Caravan.train,Caravan.test,Caravan.train$Purchase,k=5)
table(knn.pred,Caravan.test$Purchase)
mean(knn.pred==Caravan.test$Purchase)
```