# Trees
```{r prologue, results='hide', echo=FALSE}
knitr::opts_chunk$set(warning = FALSE
                      , message = FALSE
                      , echo = FALSE
                      )
```

We're going to have a look at trees, using the Carseats data
```{r setup}
require(ISLR)
require(tree)
require(lattice)
attach(Carseats)

histogram(Sales)
High <- ifelse(Sales <= 8, "No", "Yes")
Carseats <- data.frame(Carseats, High)
```

Want to fit a classification tree and need to exclude Sales as the response variable was created from Sales.

```{r model1}
tree.carseats <- tree(High~.-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

For a detailed version of the tree, it can be printed:
```{r model1.detail}
tree.carseats
```

Split into training and test
```{r train.val}
set.seed(1011)
train <- sample(1:nrow(Carseats), 250)
tree.carseats <- tree(High~.-Sales, data = Carseats, subset = train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)

tree.pred <- predict(tree.carseats
                     , Carseats[-train,]
                     , type="class")

conf.mat <- with(Carseats[-train,], table(tree.pred,High))
conf.mat
(conf.mat[1,1] + conf.mat[2,2])/sum(conf.mat)
```

Can use cross validation to recommend pruning:
```{r xval.prune}
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
best.prune <- which.min(cv.carseats$dev)
best.prune
```

And then apply the pruning function

```{r prune.tree}
prune.carseats <- prune.misclass(tree.carseats
                                 , best = cv.carseats$dev[best.prune])
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Final validation

```{r validate.pruned.tree}
tree.pred <- predict(prune.carseats
                     , Carseats[-train,]
                     , type="class")

conf.mat <- with(Carseats[-train,], table(tree.pred,High))
conf.mat
(conf.mat[1,1] + conf.mat[2,2])/sum(conf.mat)
```

This hasn't done brilliantly.

Trees can be used as building blocks in more complex models.

```{r randfor.setup}
require(randomForest)
require(MASS)
set.seed(101)
train <- sample(1:nrow(Boston), 300)
```

Will build a random forest with response medv

```{r randfor.one}
rf.boston <- randomForest(medv~., data = Boston, subset = train)
rf.boston
```

Will now look at analysing error rates
```{r randfor.xval}
oob.err <- double(13)
test.err <- double(13)
for (mtry in 1:13) {
  randfor.boston <- randomForest(medv~., data = Boston, subset = train, mtry = mtry, ntree = 400)
  
  oob.err[mtry] <- randfor.boston$mse[400]
  pred <- predict(randfor.boston, Boston[-train,])
  test.err[mtry] <- with(Boston[-train,],mean((medv-pred)^2))
  cat(mtry, " ")
}
matplot(1:mtry, cbind(test.err, oob.err)
        , pch = 19, col = c("red", "blue")
        , type = "b", ylab = "mean squared error")
legend("topright", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
```


Now for boosting

```{r boost.setup}
require(gbm)
boost.boston <- gbm(medv~., data = Boston[train,]
                    , distribution = "gaussian"
                    , n.trees = 10000
                    , shrinkage = 0.01
                    , interaction.depth = 4)
```

Can plot variable importance and the influence of specific variables:

```{r boost.analysis}
summary(boost.boston)
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

And use cross validation to find the best params
```{r boost.xval}
n.trees <- seq(100, 10000, 100)
predmat <- predict(boost.boston, Boston[-train,], n.trees = n.trees)
dim(predmat)
berr <- with(Boston[-train,], apply((predmat-medv)^2, 2, mean))

plot(n.trees, berr, pch = 19, ylab = "mean squared error")
abline(h = min(test.err), col = "red")
```

We can see this did better than the randfor

Calcultating Gini index

$$G = \sum_{k=1}^K{\hat p_{mk}}(1-{\hat p_{mk}})$$

```{r gini}
# for every class K, sum of proportion K in region * proportion not K in region

# proportions will always add to 1

p <- c(0.2, 0.8)

v <- p * (1-p)

sum(v)

```